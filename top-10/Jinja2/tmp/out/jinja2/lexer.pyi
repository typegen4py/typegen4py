from ._compat import implements_iterator as implements_iterator, intern as intern, iteritems as iteritems, text_type as text_type
from .exceptions import TemplateSyntaxError as TemplateSyntaxError
from .utils import LRUCache as LRUCache
from typing import Any, Optional

whitespace_re: Any
newline_re: Any
string_re: Any
integer_re: Any
float_re: Any
name_re: Any
check_ident: bool
TOKEN_ADD: Any
TOKEN_ASSIGN: Any
TOKEN_COLON: Any
TOKEN_COMMA: Any
TOKEN_DIV: Any
TOKEN_DOT: Any
TOKEN_EQ: Any
TOKEN_FLOORDIV: Any
TOKEN_GT: Any
TOKEN_GTEQ: Any
TOKEN_LBRACE: Any
TOKEN_LBRACKET: Any
TOKEN_LPAREN: Any
TOKEN_LT: Any
TOKEN_LTEQ: Any
TOKEN_MOD: Any
TOKEN_MUL: Any
TOKEN_NE: Any
TOKEN_PIPE: Any
TOKEN_POW: Any
TOKEN_RBRACE: Any
TOKEN_RBRACKET: Any
TOKEN_RPAREN: Any
TOKEN_SEMICOLON: Any
TOKEN_SUB: Any
TOKEN_TILDE: Any
TOKEN_WHITESPACE: Any
TOKEN_FLOAT: Any
TOKEN_INTEGER: Any
TOKEN_NAME: Any
TOKEN_STRING: Any
TOKEN_OPERATOR: Any
TOKEN_BLOCK_BEGIN: Any
TOKEN_BLOCK_END: Any
TOKEN_VARIABLE_BEGIN: Any
TOKEN_VARIABLE_END: Any
TOKEN_RAW_BEGIN: Any
TOKEN_RAW_END: Any
TOKEN_COMMENT_BEGIN: Any
TOKEN_COMMENT_END: Any
TOKEN_COMMENT: Any
TOKEN_LINESTATEMENT_BEGIN: Any
TOKEN_LINESTATEMENT_END: Any
TOKEN_LINECOMMENT_BEGIN: Any
TOKEN_LINECOMMENT_END: Any
TOKEN_LINECOMMENT: Any
TOKEN_DATA: Any
TOKEN_INITIAL: Any
TOKEN_EOF: Any
operators: Any
reverse_operators: Any
operator_re: Any
ignored_tokens: Any
ignore_if_empty: Any

def describe_token(token: Any): ...
def describe_token_expr(expr: Any): ...
def count_newlines(value: Any): ...
def compile_rules(environment: Any): ...

class Failure:
    message: Any = ...
    error_class: Any = ...
    def __init__(self, message: Any, cls: Any = ...) -> None: ...
    def __call__(self, lineno: Any, filename: Any) -> None: ...

class Token(tuple):
    lineno: Any = ...
    type: Any = ...
    value: Any = ...
    def __new__(cls, lineno: Any, type: Any, value: Any): ...
    def test(self, expr: Any): ...
    def test_any(self, *iterable: Any): ...

class TokenStreamIterator:
    stream: Any = ...
    def __init__(self, stream: Any) -> None: ...
    def __iter__(self) -> Any: ...
    def __next__(self): ...

class TokenStream:
    name: Any = ...
    filename: Any = ...
    closed: bool = ...
    current: Any = ...
    def __init__(self, generator: Any, name: Any, filename: Any) -> None: ...
    def __iter__(self) -> Any: ...
    def __bool__(self): ...
    __nonzero__: Any = ...
    @property
    def eos(self): ...
    def push(self, token: Any) -> None: ...
    def look(self): ...
    def skip(self, n: int = ...) -> None: ...
    def next_if(self, expr: Any): ...
    def skip_if(self, expr: Any): ...
    def __next__(self): ...
    def close(self) -> None: ...
    def expect(self, expr: Any): ...

def get_lexer(environment: Any): ...

class OptionalLStrip(tuple):
    def __new__(cls, *members: Any, **kwargs: Any): ...

class Lexer:
    lstrip_unless_re: Any = ...
    newline_sequence: Any = ...
    keep_trailing_newline: Any = ...
    rules: Any = ...
    def __init__(self, environment: Any): ...
    def tokenize(self, source: Any, name: Optional[Any] = ..., filename: Optional[Any] = ..., state: Optional[Any] = ...): ...
    def wrap(self, stream: Any, name: Optional[Any] = ..., filename: Optional[Any] = ...) -> None: ...
    def tokeniter(self, source: Any, name: Any, filename: Optional[Any] = ..., state: Optional[Any] = ...) -> None: ...
