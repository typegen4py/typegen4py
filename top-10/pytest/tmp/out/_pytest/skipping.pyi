from _pytest.config import Config as Config, hookimpl as hookimpl
from _pytest.config.argparsing import Parser as Parser
from _pytest.mark.structures import Mark as Mark
from _pytest.nodes import Item as Item
from _pytest.outcomes import fail as fail, skip as skip, xfail as xfail
from _pytest.reports import BaseReport as BaseReport
from _pytest.runner import CallInfo as CallInfo
from _pytest.store import StoreKey as StoreKey
from typing import Any, Generator, Optional, Tuple

def pytest_addoption(parser: Parser) -> None: ...
def pytest_configure(config: Config) -> None: ...
def evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]: ...

class Skip:
    reason: Any = ...
    def __init__(self, reason: Any) -> None: ...
    def __lt__(self, other: Any) -> Any: ...
    def __le__(self, other: Any) -> Any: ...
    def __gt__(self, other: Any) -> Any: ...
    def __ge__(self, other: Any) -> Any: ...

def evaluate_skip_marks(item: Item) -> Optional[Skip]: ...

class Xfail:
    reason: Any = ...
    run: Any = ...
    strict: Any = ...
    raises: Any = ...
    def __init__(self, reason: Any, run: Any, strict: Any, raises: Any) -> None: ...
    def __lt__(self, other: Any) -> Any: ...
    def __le__(self, other: Any) -> Any: ...
    def __gt__(self, other: Any) -> Any: ...
    def __ge__(self, other: Any) -> Any: ...

def evaluate_xfail_marks(item: Item) -> Optional[Xfail]: ...

skipped_by_mark_key: Any
xfailed_key: Any
unexpectedsuccess_key: Any

def pytest_runtest_setup(item: Item) -> None: ...
def pytest_runtest_call(item: Item) -> Generator[None, None, None]: ...
def pytest_runtest_makereport(item: Item, call: CallInfo[None]) -> Any: ...
def pytest_report_teststatus(report: BaseReport) -> Optional[Tuple[str, str, str]]: ...
